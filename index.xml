<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stephanie L. Johnson on Stephanie L. Johnson</title>
    <link>http://stephlj.github.io/index.xml</link>
    <description>Recent content in Stephanie L. Johnson on Stephanie L. Johnson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Stephanie Johnson</copyright>
    <lastBuildDate>Sun, 08 Oct 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Exploring the differences between histograms, KDEs and CDFs</title>
      <link>http://stephlj.github.io/tutorials/HistosVsCDFs/</link>
      <pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>http://stephlj.github.io/tutorials/HistosVsCDFs/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://stephlj.github.io&#34;&gt;&amp;lt; &lt;em&gt;Back to homepage&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A question we often want to ask of single-molecule data is whether the
data fall into one or more clusters. For example, perhaps we have a
single-molecule FRET system in which each molecule we observe explores
either two or three states. We might want to ask whether all of the
molecules in a population explore the same three states, or if the
molecules that explore only two states share either state with the
three-state population. Perhaps the population-averaged FRET values of
the different states, if they do form clusters, tells us something
interesting about our system.&lt;/p&gt;

&lt;p&gt;Here we compare three different ways of plotting the data to get a
sense for how the data cluster: histograms, kernel density estimation
(KDE) plots, and cumulative distribution functions (CDFs). Histograms
are the most commonly used tool for this purpose, because they are the
most intuitive, but they are the least quantitative. CDFs offer the best
tool, especially for comparisons between data sets.&lt;/p&gt;

&lt;p&gt;The code in this tutorial can be found in this
&lt;a href=&#34;https://gist.github.com/stephlj/a818b8d777092dfb08261803bf23ce2f&#34;&gt;gist&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;generate-data&#34;&gt;Generate data&lt;/h2&gt;

&lt;p&gt;We start with a synthetic data set drawn from three Gaussians,
parameterized by means &lt;code&gt;mu(i)&lt;/code&gt; and standard deviations &lt;code&gt;sigma(i)&lt;/code&gt;,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;mu    = [0.45 0.75 0.60];
sigma = [0.02 0.03 0.02];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will generate a dataset consisting of 100 samples drawn from the first Gaussian,
200 samples from the second Gaussian, and 50 samples from the third:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;n_samples = [100, 200, 50];
D = @(i) normrnd(mu(i), sigma(i), 1, n_samples(i));
data = [D(1), D(2), D(3)];
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;compare-histograms-kdes-and-cdfs&#34;&gt;Compare histograms, KDEs, and CDFs&lt;/h2&gt;

&lt;p&gt;Given a new dataset, how would you start to explore it? For a 1D-valued
dataset like this one, a common first step would be to make a histogram. If we
call Matlab&amp;rsquo;s &lt;code&gt;hist&lt;/code&gt; function with default parameters, we might be able to
guess that there are three Gaussian clusters in the data, though you could also
guess there are just two.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;figure
hist(data);  % default number and placement of bins
xlabel(&#39;Value of data&#39;);
ylabel(&#39;Counts&#39;);
set(gca,&#39;Fontsize&#39;, 14);
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://stephlj.github.io/img/HistsVsCDFs_Hist1.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Histogram of data using default parameters.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;What if we had chosen a different set of bins, say, bins 0.075 apart, starting
at 0 and ending at 1?&lt;/p&gt;

&lt;p&gt;This time, we will also normalize the histogram, so that its values sum to
1.
To normalize, we divide by the total number of counts and change the y-axis
label to frequency:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;figure
[n, xout] = hist(data, 0:0.075:1);
bar(xout, n ./ sum(n));
xlabel(&#39;Value of data&#39;);
ylabel(&#39;Frequency&#39;);
set(gca, &#39;Fontsize&#39;, 14);
xlim([0 1]);
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://stephlj.github.io/img/HistsVsCDFs_Hist2.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Histogram of data, using custom bin widths and positions.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;Consider if &lt;code&gt;data&lt;/code&gt; were a data set you had collected experimentally, and
you didn&amp;rsquo;t know how many clusters or populations to expect. How would you
know whether 10 equally spaced bins or bins at 0:0.075:1 was giving you the
&amp;ldquo;right&amp;rdquo; answer? If you had a second data set, collected under a different
condition (say, looking at activity of a mutant version of your protein
of interest), how would you know what bins to choose to best compare
them?
If you play around with the parameters of the parameters of the three Gaussians
and the bin widths and locations, you can get a sense for how much visual
variation there can be.&lt;/p&gt;

&lt;p&gt;The parameters used to construct the histogram can have a significant impact on
its visual impression, and the default of &lt;code&gt;hist&lt;/code&gt; to use 10 equally-spaced bins
is not always the best for identifying structure in any given dataset.
In fact, &lt;a href=&#34;https://www.mathworks.com/help/matlab/creating_plots/replace-discouraged-instances-of-hist-and-histc.html&#34;&gt;the use of &lt;code&gt;hist&lt;/code&gt; is now discouraged in favor of
&lt;code&gt;histogram&lt;/code&gt;&lt;/a&gt;,
in part for this reason.
The new-and-improved &lt;a href=&#34;https://www.mathworks.com/help/matlab/ref/histogram.html&#34;&gt;&lt;code&gt;histogram&lt;/code&gt;
function&lt;/a&gt; has a
data-dependent automatic binning heuristic (see e.g. the &lt;a href=&#34;https://jakevdp.github.io/blog/2012/09/12/dynamic-programming-in-python/&#34;&gt;Bayesian Blocks
algorithm&lt;/a&gt;
for a related idea). It works rather well, as shown in Figure 3:

&lt;figure &gt;
    
        &lt;img src=&#34;http://stephlj.github.io/img/HistsVsCDFs_Hist3.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Histogram of data, using Matlab&amp;#39;s histogram() function.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

However, &lt;code&gt;histogram&lt;/code&gt; still suffers from the potential for binning artifacts.&lt;/p&gt;

&lt;h3 id=&#34;kdes&#34;&gt;KDEs&lt;/h3&gt;

&lt;p&gt;An alternative to histograms is a kernel density estimate (KDE) plot.
The idea of a KDE plot is: at every data point, plot a little
Gaussian density, and then sum all the Gaussian densities together.
(This kind of KDE has a Gaussian kernel, but there are other choices of kernel
as well.)&lt;/p&gt;

&lt;p&gt;The only parameter to vary, then, is the standard deviation parameter of the
individual Gaussian densities, which is called the &lt;em&gt;bandwidth&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start with a bandwidth of 0.05:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;[f1,k1] = ksdensity(data,&#39;bandwidth&#39;,0.05);
figure
plot(x1,f1,&#39;-b&#39;,&#39;Linewidth&#39;,2)
xlabel(&#39;Value of data&#39;)
ylabel(&#39;Density&#39;)
set(gca,&#39;Fontsize&#39;,14)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 2 shows the resulting KDE. The shape of the KDE should be reminiscent of the histograms
in the previous section; they display the data in a conceptually similar way.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://stephlj.github.io/img/HistsVsCDFs_OneKDE.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Kernel density estimate  with bandwidth 0.05.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;What happens if we try a range of different bandwidths?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;bandwidths = [0.05, 0.1, 0.01, 0.001];

figure
xlabel(&#39;Value of data&#39;);
ylabel(&#39;Density&#39;)
set(gca, &#39;Fontsize&#39;, 14);
hold on
for bandwidth = bandwidths
    [f, x] = ksdensity(data, &#39;bandwidth&#39;, bandwidth);
    h = plot(x, f, &#39;-&#39;, &#39;Linewidth&#39;, 2);
    legend(h, sprintf(&#39;bw = %0.2f&#39;, bandwidth));
end
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://stephlj.github.io/img/HistsVsCDFs_KDEs.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Kernel density estimates  with various bandwidths.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;A good way of determining how robustly you can separate different
clusters of data is to vary the bandwidth, and see which peaks appear and
disappear.&lt;/p&gt;

&lt;p&gt;KDEs avoid the bin size and location details of histograms, since they have
only one bandwidth parameter to adjust, and can give a more consistent visual
impression.
Moreover, the visual impression can&amp;rsquo;t change significantly due to small changes
in the bandwidth.
But your interpretation of the data is still subject to a choice of smoothing
parameter, and it would be better to have a parameter-free visualization method.&lt;/p&gt;

&lt;h3 id=&#34;cdfs&#34;&gt;CDFs&lt;/h3&gt;

&lt;p&gt;A cumulative distribution function (CDF) measures the integral under a PDF
(probability density function) from $-\infty$ up to a particular value.
The PDF is the expression we most immediately associate with common
distributions; for example, the Gaussian distribution has a PDF of
$$ f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(- \frac{(x-\mu)^2}{2 \sigma^2} \right), $$
and so its CDF is defined as
$$ F(x) = \int_{-\infty}^x f(t) \, \mathrm{d}t. $$
The way to think about the information in a CDF is that for a value $x$,
$F(x)$ tells you how frequent a value smaller than $x$ is.&lt;/p&gt;

&lt;p&gt;CDFs have no smoothing or binning parameters to vary, and therefore provide a
good way to visualize new data or compare different datasets.
Given a data sample, we can construct an &lt;em&gt;empirical CDF&lt;/em&gt; by averaging step
functions,
$$ \widehat F(x) = \frac{1}{N} \sum_{i=1}^N I[x_i &amp;lt; x], \qquad I[a &amp;lt; b] = \begin{cases} 1 &amp;amp; a &amp;lt; b \\\ 0 &amp;amp; \text{otherwise} \end{cases}. $$
We can compute an empirical CDF in Matlab by using the &lt;a href=&#34;https://www.mathworks.com/help/stats/ecdf.html&#34;&gt;&lt;code&gt;ecdf&lt;/code&gt; function&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;[c, xc] = ecdf(data);  % computes an empirical CDF
figure
plot(xc, c, &#39;Linewidth&#39;, 2);
xlabel(&#39;Value of data&#39;);
ylabel(&#39;Cumulative probability&#39;);
set(gca, &#39;Fontsize&#39;, 14);
ylim([0 1]);
xlim([0 1]);
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://stephlj.github.io/img/HistsVsCDFs_CDF.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Cumulative probability of data.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;Figure 5 might look a bit unfamiliar, but a CDF shows us the same information as a
KDE or histogram, without the bias that might come from poorly-chosen
parameters.&lt;/p&gt;

&lt;p&gt;To gain a more intuitive understanding of what the CDF represents, we can
plot one on the same set of axes as a KDE:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;figure
ax = plotyy(x3, f3, xc, c);
xlabel(&#39;Value of data&#39;);
ylabel(ax(2), &#39;Cumulative probability&#39;);
set(ax(2), &#39;Fontsize&#39;, 14);
ylabel(ax(1), &#39;Density&#39;);
set(ax(1), &#39;Fontsize&#39;, 14);
ylim(ax(1), [0 8]);
xlim(ax(1), [0 1]);
xlim(ax(2), [0 1]);
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://stephlj.github.io/img/HistsVsCDFs_CDFvsKDE.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Cumulative probability of data compared to a KDE.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;Peaks in the KDE correspond to steep slopes in the CDF. That is, clusters in
the data appear as steep slopes, where counts are accumulating quickly in the
CDF at a certain $x$ value.&lt;/p&gt;

&lt;p&gt;Visualizing both a CDF and a KDE (or a few KDEs with different bandwidth
settings) on the same axes can give us multiple views on the same data, and let
us check that our interpretations are consistent across several views rather
than being artifacts of some particular parameter choices.
From this plot we might feel confident concluding that there are at least two
clusters in the data, and likely a third, less populated, cluster in between
the two main clusters.&lt;/p&gt;

&lt;h2 id=&#34;data-cluster-assignment&#34;&gt;Data cluster assignment&lt;/h2&gt;

&lt;p&gt;We conclude with some remarks on a related problem to the identification of clusters in a data set,
which is the
assignment of each data point to a particular cluster.
This can be a difficult task, especially when data clusters overlap (more than in
the sample data here). Often the solution in the single-molecule field is to
assign data to clusters based on a threshold value (which is usually determined
from a histogram).
For example, based on the first histogram we made, we might say that anything
with a value greater than 0.7 is in one cluster, and anything with value less
than 0.7 is in a second.
This works well for clusters that are well-separated, and has the advantage of
being relatively quick and easy.
But for less well-separated data, an alternative is to fit a mixture of PDFs
that we think describe the data well (so in our example here, a mixture of
three Gaussians), and then for each data point compute the density value that
each mixture component assigns to it.
Then we could assign each data point to the cluster which assigns highest
density to that data point.
An example is given at the end of the
&lt;a href=&#34;https://gist.github.com/stephlj/a818b8d777092dfb08261803bf23ce2f&#34;&gt;gist&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&#34;http://www.themattjohnson.com&#34;&gt;Matt Johnson&lt;/a&gt; for teaching me this material, and for
editing this tutorial for accuracy and clarity!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Are my data exponentially distributed?</title>
      <link>http://stephlj.github.io/tutorials/Exponentiality/</link>
      <pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>http://stephlj.github.io/tutorials/Exponentiality/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://stephlj.github.io&#34;&gt;&amp;lt; &lt;em&gt;Back to homepage&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To first order approximation, the results of single molecule measurements of time-dependent processes
in a biological system are usually exponentially distributed.
This is because we are often measuring the time to an event that has some
fixed, independent probability of occurring in any instant (where an instant
means a short interval of time).&lt;/p&gt;

&lt;p&gt;Think, for example, of radioactive decay: in each instant, an atom in the
sample has some fixed probability of decaying (given that it hasn&amp;rsquo;t already
decayed).
More precisely, in any given short time interval of length $\Delta t$, we&amp;rsquo;ll
say the probability of a decay occurring in that interval given that it hasn&amp;rsquo;t
already happened is $k \Delta t$, for some proportionality constant $k &amp;gt; 0.$
The average time to decay is then $\tfrac{1}{k}$, and the wait times
are &lt;a href=&#34;https://en.wikipedia.org/wiki/Exponential_distribution&#34;&gt;exponentially
distributed&lt;/a&gt; with
rate constant $k$.&lt;/p&gt;

&lt;p&gt;Keep in mind that this is different from asking &lt;em&gt;how much&lt;/em&gt; of the population
has decayed at a given point in time:
here we are interested in the &lt;em&gt;distribution of individual times to decay&lt;/em&gt;, not
the amount of un-decayed sample as a function of time.
The latter is analogous to ensemble fluorescence measurements (&lt;em&gt;e.g.&lt;/em&gt; obtained
by measuring signal from a fluorescent dye in a fluorimeter over time).&lt;/p&gt;

&lt;p&gt;Very often in biology the process we are observing consists of multiple
underlying microscopic events which we cannot observe directly.
Each of these events could be governed by an independent exponential process.
For example, an enzyme acting on its substrate would first bind the substrate,
a process described by one rate constant $k_1$ (i.e., at each time instant, the
enzyme would have some probability of binding the substrate, and $k_1$ would
have dimension of $\text{time}^{-1}$).
Then perhaps a conformational change needs to occur, a process that also has some (different)
probability per second of occurring, say $k_2$.
And then substrate would be converted to product, with yet another probability
per second of occurring, $k_3$.
If we measured the times at which product was formed for each substrate
molecule in the sample, we would find the distribution of times from substrate
to product &lt;em&gt;not&lt;/em&gt; to be exponentially distributed.
We would instead need some kind of combination of exponentials to describe the
overall process.&lt;/p&gt;

&lt;p&gt;Most often our experiments only have the resolution to differentiate between
one or two exponential processes (or perhaps most strictly, one
or more).
That is, it&amp;rsquo;s easiest to determine whether the data are not well described by a
single exponential; but whether they are best described by some combination of
two exponentials, or three, or more, is usually not possible, in the absence of
other information about the system.&lt;/p&gt;

&lt;p&gt;Therefore this tutorial will focus on distinguishing between a single
exponential distribution, and two kinds of &amp;ldquo;double&amp;rdquo; exponentials.
&amp;ldquo;Double exponential&amp;rdquo; or &amp;ldquo;bi-exponential&amp;rdquo; is an under-specified, non-technical
term frequently encountered in the single-molecule literature; it usually
refers to a &lt;a href=&#34;https://en.wikipedia.org/wiki/Hyperexponential_distribution&#34;&gt;&lt;em&gt;hyperexponential&lt;/em&gt;
distribution&lt;/a&gt;, or
&lt;a href=&#34;https://en.wikipedia.org/wiki/Mixture_distribution&#34;&gt;mixture&lt;/a&gt;, with two terms.
The other useful &amp;ldquo;double exponential&amp;rdquo; is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Hypoexponential_distribution&#34;&gt;&lt;em&gt;hypoexponential&lt;/em&gt;
distribution&lt;/a&gt;,
which models a sequential process like the enzyme reaction described above.
Although the hyperexponential is frequently used, in many cases the
hypoexponential is probably a more appropriate model for what we think is
happening in the system.&lt;/p&gt;

&lt;p&gt;The code in this tutorial can be found in &lt;a href=&#34;https://gist.github.com/stephlj/b65ee5e91df2606bf2797d04271c08d5&#34;&gt;this
gist&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;math-part-1-ways-to-combine-exponential-distributions&#34;&gt;Math, part 1: Ways to combine exponential distributions&lt;/h2&gt;

&lt;p&gt;A &lt;a href=&#34;https://en.wikipedia.org/wiki/Hypoexponential_distribution#Two_parameter_case&#34;&gt;&lt;em&gt;hypoexponential&lt;/em&gt;&lt;/a&gt;
describes the total duration of a sequential process: first one thing has to
occur, and then a second thing has to occur.
If each thing has an independent exponentially distributed duration, then
the total duration is the sum of two independent exponential random variables.
This is the more common type of process I&amp;rsquo;ve encountered.&lt;/p&gt;

&lt;p&gt;The CDF (see also &lt;a href=&#34;https://stephlj.github.io/tutorials/HistosVsCDFs&#34;&gt;Histograms vs
CDFs&lt;/a&gt;) of a two-term
hypoexponential has the form
$$
\textrm{CDFhypo}(w) = 1-\frac{k_2}{k_2-k_1}e^{-k_1w}+\frac{k_1}{k_2-k_1}e^{-k_2w},
$$
where $w$ is the wait time, and $k_1$ and $k_2$ are the rate constants that describe the two
sequential events. $k_1$ and $k_2$ have dimension $\text{time}^{-1}$.
This expression gives the probability of having a wait time of $w$ or fewer
seconds (or equivalent units). The probability density function (PDF) of this
distribution is
$$
\textrm{PDFhypo}(w) = \frac{k_1 k_2}{k_1-k_2} \left(e^{-k_2 w}-e^{-k_1 w} \right).
$$&lt;/p&gt;

&lt;p&gt;For a hypoexponential, the two rate constants that best describe the data can be estimated
from data by
$$
k_1 = \frac{2}{\mu}\left(1+\sqrt{1+2\left(\frac{\nu^2}{\mu^2}-1\right)}\right)^{-1}
$$
$$
k_2 = \frac{2}{\mu}\left(1-\sqrt{1+2\left(\frac{\nu^2}{\mu^2}-1\right)}\right)^{-1}
$$
where $\mu$ is the empirical mean of the data and $\nu$ is the standard deviation.&lt;/p&gt;

&lt;p&gt;In contrast, the
&lt;a href=&#34;https://en.wikipedia.org/wiki/Hyperexponential_distribution&#34;&gt;&lt;em&gt;hyperexponential&lt;/em&gt;&lt;/a&gt;
describes a process with a branching pathway: &lt;em&gt;either&lt;/em&gt; one thing must occur,
&lt;em&gt;or&lt;/em&gt; a different thing.
The hyperexponential is a mixture of exponentials, and its density is a
weighted sum of exponential densities.
My favorite example of this kind of process is a mixed population
in your sample.
Say, for example, you purified a multi-subunit enzyme, but part of your sample
lost subunits in the purification process.
Perhaps these lost subunits change the enzyme&amp;rsquo;s rate; then your sample of
enzyme would consist of a faster population and a slower population, and your
measurement of the wait time to product formation would be best described by a
hyperexponential.&lt;/p&gt;

&lt;p&gt;(Actually, your measurements of time to product formation would probably really best be
described by a branching pathway, with each branch having multiple sequential
events as in the hypoexponential above. But we rarely have enough data to get all this information
out of our measurements. Instead, if your mixed sample had fairly equal amounts of the two
kinds of enzyme, or if those missing subunits really made a big difference in rates, I imagine
your data would look more like a hyperexponential&amp;ndash;the rates of the two populations would dominate
over the rates of each population&amp;rsquo;s sequential processes. Conversely, if the majority of your
sample was one population, or their rates weren&amp;rsquo;t that different, your data might look more
like a hypoexponential.)&lt;/p&gt;

&lt;p&gt;The CDF of a two-term hyperexponential is given by
$$
\textrm{CDFhyper}(w) = a\left(1-e^{-k_1 w}\right)+(1-a)\left(1-e^{-k_2 w}\right),
$$
and the PDF by
$$
\textrm{PDFhyper}(w) = a k_1 e^{-k_1 w}+(1-a)k_2e^{-k_2w},
$$
where $a$ is a weighting factor that might correspond to the fraction of the first subpopulation present in the sample. Estimates of the rate constants for a hyperexponential do not have closed-form solutions,
but they can be estimated by a maximum likelihood approach as in &lt;a href=&#34;https://gist.github.com/stephlj/b65ee5e91df2606bf2797d04271c08d5&#34;&gt;this gist&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;math-part-2-distinguishing-between-combinations-of-exponentials&#34;&gt;Math, part 2: Distinguishing between combinations of exponentials&lt;/h2&gt;

&lt;p&gt;There are several approaches to determining whether your data are best described as singly
exponentially distributed, hypoexponentially distributed, or hyperexponentially distributed.&lt;/p&gt;

&lt;p&gt;One easy way is to calculate the coefficient of variation, which is the standard deviation
of the data divided by the mean. If the coefficient of variation is close to one, the data
are consistent with a single exponential; if the coefficient of variation is significantly
greater than one, the data are consistent with a hyperexponential (branching pathway);
if less than one, a hypoexponential (sequential processes). As described below, it is usually
best to bootstrap your data and re-compute the coefficient of variation for each bootstrapped
sample, and then see whether these bootstrapped coefficients of variation mostly fall around 1,
or mostly above 1, or mostly below 1.&lt;/p&gt;

&lt;p&gt;Another way of determining exponentiality is by comparing the CDF of your data to the CDFs
of a single exponential distribution, a hyperexponential distribution, and a hypoexponential
distribution, as in a P-P plot (code and example plots given below).&lt;/p&gt;

&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;

&lt;h3 id=&#34;generate-data&#34;&gt;Generate data&lt;/h3&gt;

&lt;p&gt;We will generate three synthetic data sets, one drawn from a single exponential distribution,
one from a hypoexponential, and one from a hyperexponential. The single exponential will
have a rate constant of $k_1$ = &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;5&lt;/sub&gt; per second (mean wait time of 5 seconds), and the
two-term exponentials will have rate constants $\mathit{kh}_1$ = &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;5&lt;/sub&gt; per second and $\mathit{kh}_2$ = &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;50&lt;/sub&gt; per second.
The hyperexponential will have a weighting factor of 0.3 (one population is 30% of the total).
Each data set will have 100 samples:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;k1 = 1/5;
kh = [1/50, 1/5];
a = 0.3;
n_samples = 100;
data_single = exprnd(1/k1, 1, n_samples);  % Requires the statistics toolbox
data_hypo   = exprnd(1/kh(1), 1, n_samples) + exprnd(1/kh(2), 1, n_samples);
data_hyper  = exprnd(1./kh(double(rand(1, n_samples) &amp;gt; a) + 1));
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;test-for-different-kinds-of-exponentiality&#34;&gt;Test for different kinds of exponentiality&lt;/h3&gt;

&lt;p&gt;First let&amp;rsquo;s calculate the coefficient of variation of each data set, as well as a bootstrapped
estimate of our uncertainty about that coefficient of variation:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;cv_single = std(data_single) / mean(data_single);
cv_hypo   = std(data_hypo)   / mean(data_hypo);
cv_hyper  = std(data_hyper)  / mean(data_hyper);
num_bs = 500;
cv_single_bs = bootstrp(num_bs, @(x) std(x) / mean(x), data_single);
cv_hypo_bs   = bootstrp(num_bs, @(x) std(x) / mean(x), data_hypo);
cv_hyper_bs  = bootstrp(num_bs, @(x) std(x) / mean(x), data_hyper);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(See also &lt;a href=&#34;https://stephlj.github.io/tutorials/Bootstrapping&#34;&gt;https://stephlj.github.io/tutorials/Bootstrapping&lt;/a&gt; for more information on bootstrapping.)&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m not usually a fan of histograms (see &lt;a href=&#34;https://stephlj.github.io/tutorials/HistosVsCDFs&#34;&gt;https://stephlj.github.io/tutorials/HistosVsCDFs&lt;/a&gt;),
but for a quick-and-dirty, non-quantitative look at the data, they&amp;rsquo;re ok:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;function PlotHisto(fig_handle, cv_bs, cv_mean, fig_title)
    [n, xout] = hist(cv_bs);
    n = n ./ length(cv_bs);
    subplot(fig_handle)
    bar(xout, n)
    xlabel(&#39;Coefficient of variation&#39;, &#39;Fontsize&#39;, 16)
    ylabel(&#39;Frequency&#39;, &#39;Fontsize&#39;, 16)
    title(fig_title, &#39;Fontsize&#39;, 16)
    set(gca, &#39;Fontsize&#39;, 16)
    hold on
    plot([cv_mean cv_mean], [0 max(n)], &#39;--r&#39;)
    plot([1 1], [0 max(n)], &#39;--k&#39;)
    hold off
end
figure
h1=subplot(1,3,1)
PlotHisto(h1, cv_single_bs, cv_single, &#39;Single exp&#39;)
h2=subplot(1,3,2)
PlotHisto(h2, cv_hypo_bs, cv_hypo, &#39;Hypo exp&#39;)
h3=subplot(1,3,3)
PlotHisto(h3, cv_hyper_bs, cv_hyper, &#39;Hyper exp&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 1 shows the resulting histograms of bootstrapped coefficients of variation. As expected,
the single exponential data have a coefficient of variation clustered around 1, the hypoexponential clusters
significantly below 1, and the hyperexponential clusters above 1.

&lt;figure &gt;
    
        &lt;img src=&#34;http://stephlj.github.io/img/Exponentiality_CVs.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Histograms of bootstrapped coefficients of variation for an exponential distribution, and combinations of exponentials.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Another way to assess which distribution best describes your data is to use
a P-P plot, which compares the empirical CDF of your data to the theoretical CDF of a fit distribution
you&amp;rsquo;re testing against. Code for a P-P plot in Matlab can be found in &lt;a href=&#34;https://gist.github.com/stephlj/33a909fda440c3d6743d1f133c9ad936&#34;&gt;this gist&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;How do our three data sets compare to the three distributions? See the &lt;a href=&#34;https://gist.github.com/stephlj/b65ee5e91df2606bf2797d04271c08d5&#34;&gt;gist&lt;/a&gt; for the code
that generated Figure 2:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://stephlj.github.io/img/Exponentiality_PPplots.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;P-P plots comparing synthetic data to different types of exponential distributions.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;These P-P plots give us more information than the coefficients of variation: if the
data are well-described by whatever distribution we&amp;rsquo;re testing against, as on the diagonal from
top left to bottom right, then the results of the P-P plot should fall along the $y=x$ line. If they don&amp;rsquo;t,
that tells you something about how your data differ from the theoretical distribution you&amp;rsquo;re comparing to.
For example, if the empirical data are smaller than the theoretical data at high percentiles, such that the P-P results
fall above the $y=x$ line, as in the top right panel, your data have a heavier tail than the distribution
you&amp;rsquo;re comparing to.&lt;/p&gt;

&lt;p&gt;Looking at Figure 2, we can see first of all that single exponentially distributed data are well-fit
by all three theoretical distributions. That makes sense; if we add more parameters and more terms, we
simply over-fit the data. Hyperexponentially distributed data are particularly poorly fit by a single
exponential, as indicated by the large deviation from the $y=x$ line. Note that we are unable to generate a theoretical hypoexponential distribution for the hyperexponential
data, because their coefficient of variation is greater than 1, and that means we end up with a negative
rate constant for $k_2$&amp;mdash;remember that these rate constants must be greater than zero!&lt;/p&gt;

&lt;h2 id=&#34;concluding-remarks-other-distributions-encountered-in-biology&#34;&gt;Concluding remarks: other distributions encountered in biology&lt;/h2&gt;

&lt;p&gt;Various kinds of exponential distributions are common in biology and especially in single-molecule measurements. If you&amp;rsquo;re measuring something to do with rates, or time, you&amp;rsquo;ll
likely be measuring something that is either exponentially distributed or comprises exponentially distributed components.&lt;/p&gt;

&lt;p&gt;We note briefly some contexts in which other kinds of distributions
may arise. In all of these cases, similar approaches (e.g. P-P plots) to those described
above for exponentials can be used to test whether your data are consistent with some
theoretical distribution.&lt;/p&gt;

&lt;h3 id=&#34;geometric-distributions&#34;&gt;Geometric distributions&lt;/h3&gt;

&lt;p&gt;The geometric distribution is the discretized analog to the exponential distribution. We usually
assume that the underlying natural process we are measuring is governed by the exponential
distribution, but often our measurements of said process occur in discretized increments
(for example, discretized by the frame rate of a microscope camera). Whether you use a geometric
distribution or an exponential distribution will depend on how much this discretization process
matters.&lt;/p&gt;

&lt;h3 id=&#34;gaussian&#34;&gt;Gaussian&lt;/h3&gt;

&lt;p&gt;Gaussian distributions are another common distribution in biology and in single-molecule
biophysics&amp;mdash;for example, the intensity of a stable fluorescent molecule over time will be
Gaussian-distributed around some mean value. We usually assume noise is Gaussian.&lt;/p&gt;

&lt;p&gt;Note that a common point of confusion about Gaussian-distributed samples versus exponential samples
is what the mean value of the samples represents. For a Gaussian-distributed set of measurements (in one dimension),
the most frequently observed values will cluster around the mean of the Gaussian. For exponentially distributed samples,
however, most samples will have values less than the mean.&lt;/p&gt;

&lt;h3 id=&#34;poisson-binomial&#34;&gt;Poisson/Binomial&lt;/h3&gt;

&lt;p&gt;A classic example of a binomial process in biology is dilution of a cytoplasmic factor over the
course of several generations of cell division. The distribution of how many of these factors
each daughter cell will have over time will be binomial. Alternatively,
consider how many mRNAs are produced per unit time. Notice that here we&amp;rsquo;re not interested
in measuring the &lt;em&gt;wait time to&lt;/em&gt; something, even though it sounds like there&amp;rsquo;s a time component here.
We&amp;rsquo;re really measuring &lt;em&gt;how many&lt;/em&gt; of something in a unit of time.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&#34;http://www.themattjohnson.com&#34;&gt;Matt Johnson&lt;/a&gt; for teaching me this material, and for
editing this tutorial for accuracy and clarity!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bootstrapped error calculations</title>
      <link>http://stephlj.github.io/tutorials/Bootstrapping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://stephlj.github.io/tutorials/Bootstrapping/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://stephlj.github.io&#34;&gt;&amp;lt; &lt;em&gt;Back to homepage&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Bootstrapping is a well-established method for estimating statistical errors (rather than, say, systematic or
technical errors). The idea behind the bootstrap is to ask how much the estimate of whatever
you&amp;rsquo;re interested (e.g. the mean of your data) would have changed if you&amp;rsquo;d collected a slightly
different set of data, but still drawn from the same underlying population distribution.
If your estimate is being substantially influenced by an outlier or some other
peculiarity not representative of the data set, the bootstrap can let you know.&lt;/p&gt;

&lt;p&gt;Say you have 100 samples; how much does your estimate of the mean depend on a particular 10 of them? What if
you collected samples over several days, and one day, for whatever reason, all of your sample
values from that day were a little bit larger than the rest? How much would it change your estimate of
the mean if you&amp;rsquo;d not collected data that day? A large part of statistics is about answering the question,
what estimates of my sample mean would I have obtained if I re-ran this experiment 100 or 1000
more times? The bootstrap attempts to answer this question by direct simulation.&lt;/p&gt;

&lt;p&gt;The classic reference on bootstrapping and why it works is Efron and Tibshirani (1993),
&lt;strong&gt;An introduction to the bootstrap&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;considerations&#34;&gt;Considerations&lt;/h2&gt;

&lt;p&gt;The most essential prerequisite for the bootstrapping routine is the size of your data. You
really can&amp;rsquo;t bootstrap a data set with 3 numbers in it. For the kinds of single molecule data
I&amp;rsquo;ve encountered, I&amp;rsquo;ve found 50-100 samples to be fine, but the more the better.&lt;/p&gt;

&lt;p&gt;As a corollary to the data set size, your sample data set also needs to span
a representative range of possible measurement values. Let&amp;rsquo;s say the underlying population you&amp;rsquo;re
measuring is Gaussian distributed around a mean of 20. Let&amp;rsquo;s say you make 10
measurements of this population, and all the samples happen to be concentrated
around the value 15. (Small sample sizes&amp;ndash;it can happen!) When you bootstrap an
estimator for the mean, because your sample&amp;rsquo;s variability didn&amp;rsquo;t accurately
represent the population variability, all the bootstrapped estimates might be
near 15, and you might erroneously conclude that you have low statistical
uncertainty. If we had drawn a larger dataset, this kind of fluke would be
exponentially unlikely to happen.&lt;/p&gt;

&lt;p&gt;How do you if you have &amp;ldquo;enough&amp;rdquo; data? Well, when in doubt, take more data and see if it
changes the estimate of the mean. Alternatively, you can generate multiple sub-sets of your
data, of varying size, to ask how confident your estimate of the mean would have been if you&amp;rsquo;d
collected, say, half the data. That is, sub-sample your whole data set with
replacement, generating, say, 100 new data sets that each have half the number of measurements
of your full data set. (&amp;ldquo;With replacement&amp;rdquo; means each sub-sample can have duplicates from
your original data set.) Calculate the mean value (or whatever your parameter of interest is)
for each 100 new half-sized data sets. How much does that mean value vary? What if your sub-sampled
data sets had a quarter of the data?&lt;/p&gt;

&lt;h2 id=&#34;bootstrapping-time-trajectories-or-comparable&#34;&gt;Bootstrapping time trajectories (or comparable)&lt;/h2&gt;

&lt;p&gt;Single-molecule data often take the form of time trajectories, in which something (e.g.,
FRET value) is measured over time. Often these time trajectories have multiple states
or events, and you may want to quantify something for each state. For example, say each
time trajectory explores three different states, and you quantify the mean dwell time or
mean lifetime of each state. For 100 time trajectories, each with 3 states, this would give
you 3 data sets, each with 100 numbers in them.&lt;/p&gt;

&lt;p&gt;When you bootstrap these data to estimate the error on each mean dwell time, you have to
decide whether to bootstrap over &lt;em&gt;dwell times&lt;/em&gt;, or over &lt;em&gt;trajectories&lt;/em&gt;. That is, do you
treat each of the 3 sets of dwell times independently, and bootstrap each separately?
Or do you take into account that these 3 sets of numbers may not be independent from each
other? What if there are correlations between dwells&amp;ndash;e.g., if one trajectory has a really
long first dwell, perhaps the second and third dwells are especially long as well. (Maybe
that one molecule was just really slow, at everything!)&lt;/p&gt;

&lt;p&gt;My choice is usually to bootstrap over trajectories, going with the principle of, what
if I&amp;rsquo;d happened not to collect this particular trajectory as part of my data set. Or if I&amp;rsquo;d
collected 5 of them instead of 1.&lt;/p&gt;

&lt;p&gt;Bootstrapping with respect to trajectories requires a slight modification to a standard
bootstrapping routine, as described below.&lt;/p&gt;

&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;

&lt;p&gt;If whatever software or language you&amp;rsquo;re doing data analysis in has a built-in bootstrapping
function, use that. Matlab has one (called bootstrp).&lt;/p&gt;

&lt;p&gt;The outline of a very simple bootstrapping routine would be:&lt;/p&gt;

&lt;p&gt;(1) Load your data, or generate some random data for testing purposes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;data = randn(1,87);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(2) Generate 1000 bootstrapped data sets (these days, with
computers as fast as they are, 1000 or even 10,000 is not unreasonable.
You can certainly have too few bootstrapped samples, but anything above a
couple hundred should be fine. Above a reasonable threshold, increasing
the number of bootstrapped data sets shouldn&amp;rsquo;t affect your estimate of
the error!):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;num_bs = 1000;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(3) Call Matlab&amp;rsquo;s bootstrapping function. Let&amp;rsquo;s say I&amp;rsquo;m trying to estimate the error
on the sample mean:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;means_bs = bootstrp(num_bs, @(x) mean(x), data);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternatively, to bootstrap with respect to trajectories, I
first generate a resampled-with-replacement set of trajectory
indicies&amp;ndash;that is, if I have 87 trajectories, I assign them each a
number (usually the order they load in) and then resample with
replacement the indicies. Note below that instead of bootstrapping data directly,
I&amp;rsquo;m bootstrapping indices:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;bootstat = bootstrp(num_bs, @(x) x, 1:length(data));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, each row of &lt;code&gt;bootstat&lt;/code&gt; will be a set of indices with which to resample
data with replacement (bootstat will have size &lt;code&gt;num_bs&lt;/code&gt; by &lt;code&gt;length(data)&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Then I generate new data sets based on each row of bootstat&amp;ndash;for example,
load all the trajectories corresponding to the indicies in the first row
of bootstat, then extract the durations of the first dwell for each of
these trajectories, and take the mean of those values. Then extract the
durations of the second dwell of these trajectories and calculate the mean. Etc.
Then move on to the next row of bootstat. Re-load the appropriate trajectories, extract
all their first dwell durations &amp;hellip; Eventually I would have num_bs new mean values for each
dwell time.&lt;/p&gt;

&lt;p&gt;(4) I report the standard deviation of the bootstrapped means as my estimate
of the standard error on the mean:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;err = std(means_bs);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&#34;http://www.themattjohnson.com&#34;&gt;Matt Johnson&lt;/a&gt; for teaching me this material, and for
editing this tutorial for accuracy and clarity!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
