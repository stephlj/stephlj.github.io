<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stephanie L. Johnson on Stephanie L. Johnson</title>
    <link>http://stephlj.github.io/index.xml</link>
    <description>Recent content in Stephanie L. Johnson on Stephanie L. Johnson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Exploring the differences between histograms, KDEs and CDFs</title>
      <link>http://stephlj.github.io/tutorials/HistosVsCDFs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://stephlj.github.io/tutorials/HistosVsCDFs/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://stephlj.github.io&#34;&gt;&amp;lt; &lt;em&gt;Back to homepage&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A question we often want to ask of single-molecule data is whether the
data fall into one or more clusters. For example, perhaps we have a
single-molecule FRET system in which each molecule we observe explores
either two or three states. We might want to ask whether all of the
molecules in a population explore the same three states, or if the
molecules that explore only two states share either state with the
three-state population. Perhaps the population-averaged FRET values of
the different states, if they do form clusters, tells us something
interesting about our system.&lt;/p&gt;

&lt;p&gt;Here we will compare three different ways of plotting the data to get a
sense for how the data cluster: histograms, kernel density estimation
(KDE) plots, and cumulative distribution functions (CDFs). Histograms
are the most commonly used tool for this purpose, because they are the
most intuitive, but they are the least quantitative. CDFs offer the best
tool, especially for comparisons between data sets, but are less
intuitive.&lt;/p&gt;

&lt;p&gt;The code in this tutorial can be found in this &lt;a href=&#34;https://gist.github.com/stephlj/a818b8d777092dfb08261803bf23ce2f&#34;&gt;gist&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;generate-data&#34;&gt;Generate data&lt;/h2&gt;

&lt;p&gt;We will start with a synthetic data set drawn from three Gaussians, parameterized by
mu1 = 0.45, std1 = 0.02, mu2 = 0.75, std2 = 0.03, mu3 = 0.6, std3 = 0.02, where each
mu represents the mean of one of the Gaussians and std is the standard deviation.&lt;/p&gt;

&lt;p&gt;Our dataset will consist of 100 samples drawn from the first Gaussian, 200 samples from the
second Gaussian, and 50 samples from the third Gaussian. To draw these samples, we will generate
100 random numbers uniformly sampled from the interval [0,1] and compute the inverse CDF
of our Gaussian distrubtion for each 100 numbers (see below for a description of CDFs):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;mu1 = 0.45;
std1 = 0.02;
n_samples1 = 100;
InvCDF = @(x,mu,std)(mu+std.*sqrt(2).*erfinv(2.*x-1));
n = rand(1,n_samples1);
data_cluster_1 = InvCDF(n,mu1,std1)+randn(1,n_samples1)./50;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that in the last line, we use randn to add some noise, so that our synthetic data
aren&amp;rsquo;t perfectly Gaussian distributed. We repeat the same but with mu2, std2, mu3, and std3
as given above, and with n_samples2 = 200 and n_samples3 = 50.&lt;/p&gt;

&lt;p&gt;Finally we combine these three samplings to obtain our synthetic dataset:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;data = [data_cluster_1,data_cluster_2,data_cluster_3];
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;compare-histograms-kdes-and-cdfs-of-data&#34;&gt;Compare histograms, KDEs, and CDFs of data&lt;/h2&gt;

&lt;p&gt;If we call Matlab&amp;rsquo;s hist function with default parameters, we can observe
our three Gaussian clusters in the data (though deciding between two and
three clusters, if we didn&amp;rsquo;t know there were three to start with, might
be difficult; and it also depends on the outcome of the calls to randn in
generating &lt;data&gt;). For example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;figure
hist(data)
xlabel(&#39;Value of data&#39;)
ylabel(&#39;Counts&#39;)
set(gca,&#39;Fontsize&#39;,14)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;generates the following figure (again, subject to the noise generated by randn):&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://stephlj.github.io/img/HistsVsCDFs_Hist1.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Histogram of &amp;lt;data&amp;gt;, using Matlab&amp;#39;s default parameters.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;But what if we had chosen a different set of bins, say, bins 0.075 apart,
starting at 0 and ending at 1?&lt;/p&gt;

&lt;p&gt;First, we note a nice feature of histograms, not shared by the KDEs described below, is
that they can be normalized, so that they integrate to 1. To normalize, we divide by the
total number of counts (sum(n)). But then we must change the y-axis label to frequency,
not counts:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;figure
[n,xout] = hist(data,0:0.075:1);
n = n./sum(n);
bar(xout,n)
xlabel(&#39;Value of data&#39;)
ylabel(&#39;Frequency&#39;)
set(gca,&#39;Fontsize&#39;,14)
xlim([0 1])
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://stephlj.github.io/img/HistsVsCDFs_Hist2.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Histogram of &amp;lt;data&amp;gt;, using custom bin widths and positions.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;Note: It is not the case that Matlab&amp;rsquo;s default parameters choose the best bins
to separate clusters in the data. Matlab always defaults to 10 equally
spaced bins from min(data) to max(data). If you&amp;rsquo;re unconvinced that
this default is not always the best choice, play around with the
parameters of the three Gaussians, the amount of noise, and the bin
widths and locations.&lt;/p&gt;

&lt;p&gt;Consider if &lt;data&gt; were a data set you had collected experimentally, and
you didn&amp;rsquo;t know how many clusters or populations to expect. How would you
know whether 10 equally spaced bins or bins at 0:0.1:1 was giving you the
&amp;ldquo;right&amp;rdquo; answer? If you had a second data set, collected under a different
condition (say, looking at activity of a mutant version of your protein
of interest), how would you know what bins to choose to best compare
them?&lt;/p&gt;

&lt;h3 id=&#34;kdes&#34;&gt;KDEs:&lt;/h3&gt;

&lt;p&gt;An alternative to histograms is a kernel density estimation (KDE) plot.
The idea of a KDE is: for every data point, plot a little Gaussian whose
mean is the value of that data point, and then sum all the Gaussians
together. (This kind of KDE has a Gaussian kernel; you can use a
different distribution as your kernel). The only parameter to vary, then,
is the standard deviation of the little Gaussians, which is called the
bandwidth.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start with a bandwidth of 0.05:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;[f1,x1] = ksdensity(data,&#39;bandwidth&#39;,0.05);
figure
plot(x1,f1,&#39;-b&#39;,&#39;Linewidth&#39;,2)
xlabel(&#39;Value of data&#39;)
ylabel(&#39;Density&#39;) 
set(gca,&#39;Fontsize&#39;,14)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As noted in passing above, KDEs are not normalized, which makes
quantitative comparisions between data sets difficult. Thus the y-label of &amp;ldquo;Density&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;What happens when we vary the bandwidth?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;[f2,x2] = ksdensity(data,&#39;bandwidth&#39;,0.1);
[f3,x3] = ksdensity(data,&#39;bandwidth&#39;,0.01);
[f4,x4] = ksdensity(data,&#39;bandwidth&#39;,0.001);
hold on
plot(x2,f2,&#39;-r&#39;,&#39;Linewidth&#39;,2)
plot(x3,f3,&#39;-g&#39;,&#39;Linewidth&#39;,2)
plot(x4,f4,&#39;-k&#39;)
legend(&#39;bw = 0.05&#39;,&#39;bw = 0.1&#39;,&#39;bw = 0.01&#39;,&#39;bw = 0.001&#39;)
xlim([0 1])
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://stephlj.github.io/img/HistsVsCDFs_KDEs.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Kernel density estimations of &amp;lt;data&amp;gt; with various bandwidths.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;A good way of determining how robustly you can separate different
clusters of data is to vary the bandwidth, and see which peaks appear and
disappear.&lt;/p&gt;

&lt;p&gt;But your interpretation of your data is still subject to a smoothing
parameter.&lt;/p&gt;

&lt;h3 id=&#34;cdfs&#34;&gt;CDFs&lt;/h3&gt;

&lt;p&gt;A cumulative density function (CDF) measures the integral under a PDF
(probability density function) from -Inf up to a certain value.
The PDF is the expression we usually associate with common distributions,
like the Gaussian (normal) distribution, whose PDF is P(x) = 1/sqrt(pi*c)*exp(-(x-mu)^2/c).
The way to think about the information in a CDF is that for a value x,
CDF(x) tells you how many counts have that x value or a smaller x value.&lt;/p&gt;

&lt;p&gt;CDFs have no smoothing or binning parameters to vary, and therefore
represent a good way to compare between data sets:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;[c,xc] = ecdf(data);
figure
plot(xc,c,&#39;Linewidth&#39;,2)
xlabel(&#39;Value of data&#39;)
ylabel(&#39;Cumulative probability&#39;)
set(gca,&#39;Fontsize&#39;,14)
ylim([0 1])
xlim([0 1])
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://stephlj.github.io/img/HistsVsCDFs_CDF.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Cumulative probability of &amp;lt;data&amp;gt;.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;To gain a more intuitive understanding of what the CDF represents, we can
plot them on the same set of axes as a KDE:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;figure
ax = plotyy(x3,f3,xc,c);
xlabel(&#39;Value of data&#39;)
ylabel(ax(2),&#39;Cumulative probability&#39;)
set(ax(2),&#39;Fontsize&#39;,14)
ylabel(ax(1),&#39;Density&#39;)
set(ax(1),&#39;Fontsize&#39;,14)
ylim(ax(1),[0 8])
xlim(ax(1),[0 1])
xlim(ax(2),[0 1])
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://stephlj.github.io/img/HistsVsCDFs_CDFvsKDE.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Cumulative probability of &amp;lt;data&amp;gt; compared to a KDE.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;Note that peaks in the KDEs become steep slopes in the CDFs. That is,
clusters in the data appear as steep slopes, where counts are
accumulating quickly in the CDF at a certain x value. From this plot we might feel confident
concluding that there are at least two clusters in the data, and likely a third, less
populated, cluster in between the two main clusters.&lt;/p&gt;

&lt;h2 id=&#34;data-cluster-assignment&#34;&gt;Data cluster assignment&lt;/h2&gt;

&lt;p&gt;As a concluding remark, we note that a related problem to the identification of clusters
in a data set is the assignment of each data point to a particular cluster. This is a difficult
task, especially when data clusters overlap (more than in the sample data here). Often
the solution in the single-molecule field is to assign data to clusters based on a threshold
value (which is usually determined from a histogram). For example, based on the first histogram,
we might say that anything with a value greater than 0.7 is in one cluster, and anything
with value less than 0.7 is in a second. This works well for clusters that are well-separated,
and has the advantage of being relatively quick and easy. However, for less well-separated data,
an alternative would be to fit a combination of PDFs that we think describe the data well
(so in our example here, three Gaussians) to the data set, and then for every data point,
compute the probability that a particular value was drawn from each of the three Gaussians. Then we
could assign each data point to the cluster for which its probability mass was largest. An
example is given at the end of the &lt;a href=&#34;https://gist.github.com/stephlj/a818b8d777092dfb08261803bf23ce2f&#34;&gt;gist&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
